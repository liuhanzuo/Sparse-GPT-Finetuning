# æé™å‹ç¼©å®éªŒ - åŠŸèƒ½å¢å¼ºè¯´æ˜

æœ¬æ–‡æ¡£è¯´æ˜äº†å¯¹æé™å‹ç¼©å®éªŒè„šæœ¬çš„æ‰€æœ‰åŠŸèƒ½å¢å¼ºã€‚

## ğŸ¯ ä¸»è¦åŠŸèƒ½

### 1. **è‡ªåŠ¨ Perplexity è¯„ä¼°**

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè‡ªåŠ¨åœ¨ä»¥ä¸‹æ—¶é—´ç‚¹è¯„ä¼° Perplexityï¼š

- âœ… **è®­ç»ƒå‰åŸºçº¿è¯„ä¼°**ï¼šåœ¨ç¬¬ä¸€ä¸ª epoch å¼€å§‹å‰è¯„ä¼°åŸå§‹æ¨¡å‹
- âœ… **æ¯ä¸ª epoch åè¯„ä¼°**ï¼šæ¯ä¸ªè®­ç»ƒ epoch ç»“æŸåç«‹å³è¯„ä¼°
- âœ… **è¶‹åŠ¿è·Ÿè¸ª**ï¼šå®æ—¶æ˜¾ç¤ºç›¸å¯¹åŸºçº¿çš„æ”¹è¿›ç™¾åˆ†æ¯”
- âœ… **æœ€ç»ˆæ€»ç»“**ï¼šè®­ç»ƒç»“æŸåæ˜¾ç¤ºæœ€ä½³/æœ€ç»ˆ perplexity

### 2. **æ™ºèƒ½æ‰¹å¤§å°ç®¡ç†**

æ ¹æ®æ‰¹å¤§å°è‡ªåŠ¨è°ƒæ•´è®­ç»ƒæ­¥æ•°å’Œå­¦ä¹ ç‡ï¼š

```python
# ç¤ºä¾‹ï¼šä¿æŒæ€»æ•°æ®é‡ä¸å˜
batch_size=1  â†’ max_step=10000, lr=5e-6
batch_size=4  â†’ max_step=2500,  lr=1e-5
batch_size=8  â†’ max_step=1250,  lr=1.41e-5
```

**å…¬å¼**ï¼š
- `max_step = 10000 / batch_size`
- `lr = base_lr * sqrt(batch_size)`

### 3. **è¯¦ç»†è®­ç»ƒæ—¥å¿—**

æ¯ä¸ª epoch æ˜¾ç¤ºï¼š
- ğŸ“Š å¹³å‡ Loss
- ğŸ”¢ å¤„ç†çš„æ‰¹æ¬¡æ•°
- ğŸ’¾ GPU æ˜¾å­˜ä½¿ç”¨ç»Ÿè®¡
- ğŸ“ˆ Perplexity å˜åŒ–è¶‹åŠ¿

### 4. **å‘½ä»¤è¡Œå‚æ•°**

```powershell
# åŸºç¡€ç”¨æ³•
python .\extreme_compression_experiment.py --sparsity 0.1

# æŒ‡å®šæ‰¹å¤§å°ï¼ˆåŠ é€Ÿè®­ç»ƒï¼‰
python .\extreme_compression_experiment.py --sparsity 0.1 --batch-size 4

# å®Œæ•´ç¤ºä¾‹
python .\extreme_compression_experiment.py \
    --sparsity 0.1 \
    --model opt-350m \
    --batch-size 8
```

## ğŸ“Š è¾“å‡ºç¤ºä¾‹

### è®­ç»ƒå‰åŸºçº¿è¯„ä¼°

```
======================================================================
ğŸ“Š Baseline Evaluation (Before Training)
======================================================================
  ğŸ“– Loading Wikitext-2 test set...
  ğŸ“ Sequence length: 245566
  ğŸ§® Computing perplexity: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120
  âœ… Baseline Perplexity: 27.45
======================================================================
```

### Epoch ç»“æŸè¯„ä¼°

```
======================================================================
âœ… Epoch 1 Complete
======================================================================
  ğŸ“Š Average Loss: 3.2145
  ğŸ”¢ Batches Processed: 10000

ğŸ’¾ Memory Stats:
  Memory before entering the train: 478 MB
  Memory consumed (end-begin): 1279 MB
  Peak Memory consumed (max-begin): 2260 MB
  Total Peak Memory (max): 2738 MB

ğŸ” Evaluating model after Epoch 1...
======================================================================
ğŸ“Š Evaluation After Epoch 1
======================================================================
  ğŸ“– Loading Wikitext-2 test set...
  ğŸ§® Computing perplexity: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120
  âœ… Perplexity: 25.82
  ğŸ“ˆ Epoch: 1/5
  ğŸ¯ Sparsity: 0.1
======================================================================

ğŸ“ˆ Perplexity Trend:
  Baseline: 27.45
  Epoch 1: 25.82 (â†“5.9%)
```

### æœ€ç»ˆæ€»ç»“

```
======================================================================
ğŸ“Š Final Perplexity Summary
======================================================================
  Baseline: 27.45
  Final (Epoch 5): 23.67
  Best (Epoch 4): 23.51
  Total Improvement: +13.8%
======================================================================
```

## ğŸ”§ æŠ€æœ¯ç»†èŠ‚

### Perplexity è®¡ç®—

åŸºäº `Testing.ipynb` çš„å®ç°ï¼š

```python
def evaluate_perplexity(model, model_name, device='cuda', 
                       token_length=2048, stride=2048):
    """
    ä½¿ç”¨æ»‘åŠ¨çª—å£åœ¨ Wikitext-2 æµ‹è¯•é›†ä¸Šè®¡ç®— perplexity
    
    - Token length: 2048
    - Stride: 2048 (æ— é‡å )
    - Dataset: Wikitext-2-raw-v1 test split
    """
```

### è¯„ä¼°æ—¶æœº

1. **è®­ç»ƒå‰**ï¼š
   ```python
   baseline_ppl = evaluate_and_log(
       model=model,
       epoch=None,  # None = baseline
       ...
   )
   ```

2. **æ¯ä¸ª epoch å**ï¼š
   ```python
   for epoch in range(num_epochs):
       # ... training ...
       
       epoch_ppl = evaluate_and_log(
           model=model,
           epoch=epoch,
           ...
       )
   ```

### æ˜¾å­˜ç®¡ç†

- è¯„ä¼°æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ° `model.eval()` æ¨¡å¼
- ä½¿ç”¨ `torch.no_grad()` å‡å°‘æ˜¾å­˜å ç”¨
- è¯„ä¼°åè‡ªåŠ¨åˆ‡å› `model.train()` æ¨¡å¼

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

### ä¸åŒæ‰¹å¤§å°çš„è®­ç»ƒæ—¶é—´ï¼ˆä¼°ç®—ï¼‰

| Batch Size | Steps/Epoch | è®­ç»ƒæ—¶é—´ | è¯„ä¼°æ—¶é—´ | æ€»æ—¶é—´ | æ˜¾å­˜éœ€æ±‚ |
|-----------|------------|---------|---------|--------|---------|
| 1 | 10000 | 50min | 3min | ~53min | ~2GB |
| 2 | 5000 | 30min | 3min | ~33min | ~3GB |
| 4 | 2500 | 18min | 3min | ~21min | ~5GB |
| 8 | 1250 | 12min | 3min | ~15min | ~8GB |

> æ³¨æ„ï¼šè¯„ä¼°æ—¶é—´å¯¹æ‰€æœ‰é…ç½®ç›¸åŒï¼Œå› ä¸ºè¯„ä¼°ä¸å—è®­ç»ƒæ‰¹å¤§å°å½±å“

## ğŸ“ ä½¿ç”¨å»ºè®®

### 1. ä½æ˜¾å­˜åœºæ™¯ï¼ˆâ‰¤4GBï¼‰
```powershell
python .\extreme_compression_experiment.py --sparsity 0.1 --batch-size 1
```

### 2. ä¸­ç­‰æ˜¾å­˜åœºæ™¯ï¼ˆ6-8GBï¼‰
```powershell
python .\extreme_compression_experiment.py --sparsity 0.1 --batch-size 4
```

### 3. é«˜æ˜¾å­˜åœºæ™¯ï¼ˆâ‰¥12GBï¼‰
```powershell
python .\extreme_compression_experiment.py --sparsity 0.1 --batch-size 8
```

### 4. å¿«é€Ÿæµ‹è¯•ï¼ˆå‡å°‘æ­¥æ•°ï¼‰

ç¼–è¾‘ `extreme_compression_experiment.py`ï¼š
```python
base_samples = 1000  # ä» 10000 æ”¹ä¸º 1000
config["num_epochs"] = 2  # ä» 5 æ”¹ä¸º 2
```

## ğŸ“ å®éªŒæ—¥å¿—

æ‰€æœ‰ perplexity ç»“æœä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®æ—¶æ˜¾ç¤ºï¼Œå»ºè®®ï¼š

1. **é‡å®šå‘åˆ°æ–‡ä»¶**ï¼š
   ```powershell
   python .\extreme_compression_experiment.py --sparsity 0.1 2>&1 | Tee-Object -FilePath training_log.txt
   ```

2. **ä½¿ç”¨ WandB è·Ÿè¸ª**ï¼ˆéœ€è¦åœ¨ `fsdp_finetune.py` ä¸­å¯ç”¨ `--with_tracking`ï¼‰

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ï¼šè¯„ä¼°æ—¶æ˜¾å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**ï¼šå‡å° `token_length`

ç¼–è¾‘ `utils/eval_utils.py`ï¼š
```python
def evaluate_perplexity(..., token_length=1024, stride=1024):  # ä» 2048 æ”¹ä¸º 1024
```

### é—®é¢˜ï¼šè¯„ä¼°æ—¶é—´è¿‡é•¿

**è§£å†³æ–¹æ¡ˆ**ï¼šå¢å¤§ `stride`

```python
def evaluate_perplexity(..., stride=4096):  # ä» 2048 æ”¹ä¸º 4096
```
è¿™ä¼šå‡å°‘è®¡ç®—çª—å£æ•°é‡ï¼Œä½†å¯èƒ½ç•¥å¾®å½±å“ç²¾åº¦ã€‚

### é—®é¢˜ï¼šBaseline perplexity å¼‚å¸¸é«˜

**å¯èƒ½åŸå› **ï¼š
1. å‰ªææ¨¡å‹åŠ è½½å¤±è´¥
2. ç¨€ç–åº¦è®¾ç½®è¿‡é«˜ï¼ˆå¦‚ 0.05ï¼‰
3. æ¨¡å‹æ¶æ„ä¸åŒ¹é…

**æ£€æŸ¥æ–¹æ³•**ï¼š
```python
# åœ¨è¯„ä¼°å‰æ‰“å°éé›¶å‚æ•°æ¯”ä¾‹
for name, param in model.named_parameters():
    if 'weight' in name:
        nonzero_ratio = (param != 0).float().mean()
        print(f"{name}: {nonzero_ratio:.2%} non-zero")
```

## ğŸ“š ç›¸å…³æ–‡ä»¶

- `extreme_compression_experiment.py` - ä¸»å®éªŒè„šæœ¬
- `fsdp_finetune.py` - FSDP è®­ç»ƒæ ¸å¿ƒé€»è¾‘
- `utils/eval_utils.py` - Perplexity è¯„ä¼°å·¥å…·
- `Testing.ipynb` - åŸå§‹è¯„ä¼°å‚è€ƒå®ç°

## ğŸ”„ æœªæ¥æ”¹è¿›

- [ ] æ”¯æŒæ›´å¤šè¯„ä¼°æŒ‡æ ‡ï¼ˆBLEU, ROUGE ç­‰ï¼‰
- [ ] æ·»åŠ æ—©åœï¼ˆEarly Stoppingï¼‰åŸºäº perplexity
- [ ] æ”¯æŒè‡ªå®šä¹‰è¯„ä¼°æ•°æ®é›†
- [ ] æ·»åŠ  TensorBoard å¯è§†åŒ–
- [ ] è‡ªåŠ¨ä¿å­˜æœ€ä½³ perplexity checkpoint

---

**æœ€åæ›´æ–°**ï¼š2025å¹´10æœˆ21æ—¥
