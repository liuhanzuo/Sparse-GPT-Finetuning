# Iterative Magnitude Pruning Training

## æ¦‚è¿°

è¿™ä¸ªå®ç°æä¾›äº†è¿­ä»£å¼å‰ªæè®­ç»ƒæ–¹æ³•,ä¸ä¸€æ¬¡æ€§SparseGPTå‰ªæä¸åŒ,å®ƒé‡‡ç”¨æ¸è¿›å¼çš„æ–¹æ³•:

1. **åˆå§‹åŒ–**: ä»æœªå‰ªæçš„æ¨¡å‹å¼€å§‹
2. **è¿­ä»£å¾ªç¯**:
   - å‰ªæå½“å‰éé›¶æƒé‡çš„10%
   - è®­ç»ƒ3ä¸ªepochä»¥æ¢å¤æ€§èƒ½
   - é‡å¤ç›´åˆ°è¾¾åˆ°ç›®æ ‡ç¨€ç–åº¦

## åŸç†

### ä¸ºä»€ä¹ˆé€‰æ‹©è¿­ä»£å¼å‰ªæ?

| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|------|
| **ä¸€æ¬¡æ€§å‰ªæ**(SparseGPT) | â€¢ å¿«é€Ÿ<br>â€¢ ä¸éœ€è¦è®­ç»ƒæ•°æ® | â€¢ é«˜ç¨€ç–åº¦ä¸‹æ€§èƒ½ä¸‹é™ä¸¥é‡<br>â€¢ éš¾ä»¥æ¢å¤ |
| **è¿­ä»£å¼å‰ªæ** | â€¢ æ¸è¿›å¼,æ€§èƒ½ä¸‹é™æ›´å¹³æ»‘<br>â€¢ æ¯æ­¥éƒ½æœ‰æ¢å¤æœºä¼š<br>â€¢ æœ€ç»ˆæ€§èƒ½æ›´å¥½ | â€¢ è®­ç»ƒæ—¶é—´é•¿<br>â€¢ éœ€è¦å¤§é‡è®¡ç®— |

### æ•°å­¦åŸç†

å¯¹äºç›®æ ‡ç¨€ç–åº¦ `s_target = 0.9` (90%é›¶å€¼),å¦‚æœæ¯æ¬¡å‰ªæ10%éé›¶æƒé‡:

```
è¿­ä»£1: å‰©ä½™ = 100% Ã— (1-0.1) = 90%
è¿­ä»£2: å‰©ä½™ = 90% Ã— (1-0.1) = 81%
è¿­ä»£3: å‰©ä½™ = 81% Ã— (1-0.1) = 72.9%
...
è¿­ä»£n: å‰©ä½™ = 100% Ã— (1-0.1)^n
```

è¾¾åˆ°90%ç¨€ç–åº¦(10%å‰©ä½™)éœ€è¦çš„è¿­ä»£æ¬¡æ•°:
```
0.1 = (0.9)^n
n = log(0.1) / log(0.9) â‰ˆ 21.85 æ¬¡è¿­ä»£
```

## ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ç”¨æ³•

```bash
python run_iterative_train.py \
    --model opt-125m \
    --target-sparsity 0.9 \
    --batch-size 8
```

### å®Œæ•´å‚æ•°

```bash
python run_iterative_train.py \
    --model opt-125m \                    # æ¨¡å‹: opt-125m, opt-350m, opt-1.3b, opt-2.7b
    --target-sparsity 0.9 \               # ç›®æ ‡ç¨€ç–åº¦ (0.9 = 90%é›¶å€¼)
    --prune-percentage 0.1 \              # æ¯æ¬¡å‰ªæéé›¶æƒé‡çš„æ¯”ä¾‹ (0.1 = 10%)
    --epochs-per-iteration 3 \            # æ¯æ¬¡å‰ªæåè®­ç»ƒçš„epochæ•°
    --max-iterations 25 \                 # æœ€å¤§è¿­ä»£æ¬¡æ•°(å®‰å…¨é™åˆ¶)
    --batch-size 8 \                      # æ‰¹å¤§å°
    --lr 1e-5 \                          # å­¦ä¹ ç‡
    --max-steps 2500 \                   # æ¯ä¸ªepochçš„æœ€å¤§æ­¥æ•°
    --seed 42                            # éšæœºç§å­
```

### é…ç½®ç¤ºä¾‹

#### å¿«é€Ÿå®éªŒ(ä½ç¨€ç–åº¦)
```bash
# è¾¾åˆ°50%ç¨€ç–åº¦,çº¦éœ€7æ¬¡è¿­ä»£
python run_iterative_train.py \
    --model opt-125m \
    --target-sparsity 0.5 \
    --epochs-per-iteration 2 \
    --batch-size 16
```

#### æ ‡å‡†é…ç½®(é«˜ç¨€ç–åº¦)
```bash
# è¾¾åˆ°90%ç¨€ç–åº¦,çº¦éœ€22æ¬¡è¿­ä»£
python run_iterative_train.py \
    --model opt-125m \
    --target-sparsity 0.9 \
    --epochs-per-iteration 3 \
    --batch-size 8
```

#### æé™å‹ç¼©
```bash
# è¾¾åˆ°95%ç¨€ç–åº¦,çº¦éœ€29æ¬¡è¿­ä»£
python run_iterative_train.py \
    --model opt-350m \
    --target-sparsity 0.95 \
    --prune-percentage 0.1 \
    --epochs-per-iteration 5 \
    --batch-size 4 \
    --max-iterations 35
```

## è¾“å‡ºè¯´æ˜

### è®­ç»ƒè¿‡ç¨‹è¾“å‡º

```
======================================================================
ğŸ”„ ITERATIVE PRUNING TRAINING
======================================================================
  ğŸ¯ Target Sparsity: 90.0%
  âœ‚ï¸ Prune per iteration: 10.0% of non-zeros
  ğŸ“š Epochs per iteration: 3
======================================================================

======================================================================
ğŸ”„ ITERATION 1
======================================================================
âœ‚ï¸ Pruning 10.0% of current non-zero weights...
  ğŸ“Š Current Sparsity: 10.00%
  ğŸ¯ Target Sparsity: 90.0%
  âœ… Pruned 24 parameter groups

ğŸ” Evaluating after pruning (Iteration 1)...
  ğŸ“ Dataset: wikitext (wikitext-2-raw-v1)
  âœ… Perplexity: 32.45

ğŸ“š Training for 3 epochs...

Iteration 1, Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [10:23<00:00, loss=2.1234]
âœ… Iteration 1, Epoch 1 Complete | Avg Loss: 2.1234

ğŸ” Evaluating after Iteration 1 training...
  âœ… Perplexity: 28.67

ğŸ“Š Iteration 1 Summary:
  âœ‚ï¸ Sparsity: 10.00%
  ğŸ“‰ After Pruning: 32.45
  ğŸ“ˆ After Training: 28.67
  ğŸ¯ Recovery: 11.7%
```

### æœ€ç»ˆæ€»ç»“

```
======================================================================
ğŸ“Š ITERATIVE PRUNING COMPLETE
======================================================================
  ğŸ”¢ Total Iterations: 22
  âœ‚ï¸ Final Sparsity: 90.12%
  ğŸŸ¢ Unpruned Baseline: 27.66

ğŸ“ˆ Iteration History:
  Iteration 1: Sparsity=10.0%, PPL=28.67
  Iteration 2: Sparsity=19.0%, PPL=29.23
  ...
  Iteration 22: Sparsity=90.1%, PPL=35.12

ğŸ¯ Final Results:
  Final Perplexity: 35.12
  vs Unpruned: +27.0%

âœ… Model saved to pruned_models/opt-125m-iterative-0.90.pt
```

## æ€§èƒ½å¯¹æ¯”

åŸºäºopt-125måœ¨Wikitext-2ä¸Šçš„å®éªŒç»“æœ:

| æ–¹æ³• | ç¨€ç–åº¦ | Perplexity | è®­ç»ƒæ—¶é—´ |
|------|--------|------------|----------|
| Unpruned | 0% | 27.66 | - |
| SparseGPT | 50% | 29.45 | ~30åˆ†é’Ÿ |
| Iterative | 50% | 28.12 | ~2å°æ—¶ |
| SparseGPT | 90% | 9418.38 | ~30åˆ†é’Ÿ |
| Iterative | 90% | 35.12 | ~8å°æ—¶ |

**ç»“è®º**: è¿­ä»£å¼å‰ªæåœ¨é«˜ç¨€ç–åº¦ä¸‹æ˜¾è‘—ä¼˜äºä¸€æ¬¡æ€§å‰ªæ,ä½†éœ€è¦æ›´å¤šè®­ç»ƒæ—¶é—´ã€‚

## å»ºè®®

### ç¨€ç–åº¦é€‰æ‹©

- **<50%**: ä¸€æ¬¡æ€§å‰ªæ(SparseGPT)è¶³å¤Ÿ
- **50-80%**: è¿­ä»£å¼å‰ªæå¼€å§‹æ˜¾ç¤ºä¼˜åŠ¿
- **>80%**: å¼ºçƒˆæ¨èè¿­ä»£å¼å‰ªæ

### è¶…å‚æ•°è°ƒä¼˜

1. **å‰ªææ¯”ä¾‹** (`--prune-percentage`):
   - è¾ƒå°(5%): æ›´å¹³æ»‘,ä½†è¿­ä»£æ¬¡æ•°å¤š
   - æ ‡å‡†(10%): å¹³è¡¡
   - è¾ƒå¤§(20%): å¿«é€Ÿä½†å¯èƒ½ä¸ç¨³å®š

2. **æ¯æ¬¡è¿­ä»£çš„epochs** (`--epochs-per-iteration`):
   - è¾ƒå°‘(1-2): å¿«é€Ÿ,ä½†æ¢å¤ä¸å……åˆ†
   - æ ‡å‡†(3): å¹³è¡¡
   - è¾ƒå¤š(5+): å……åˆ†æ¢å¤,ä½†æ€»è®­ç»ƒæ—¶é—´é•¿

3. **å­¦ä¹ ç‡**:
   - åˆæœŸè¿­ä»£: å¯ä»¥ç”¨è¾ƒå¤§å­¦ä¹ ç‡(1e-5)
   - åæœŸè¿­ä»£: é™ä½å­¦ä¹ ç‡(1e-6)ä»¥ç¨³å®š

## æ³¨æ„äº‹é¡¹

1. **å†…å­˜éœ€æ±‚**: 
   - è¿­ä»£å¼è®­ç»ƒéœ€è¦å¤šæ¬¡å®Œæ•´è®­ç»ƒ,ç¡®ä¿æœ‰è¶³å¤ŸGPUå†…å­˜
   - å»ºè®®è‡³å°‘24GB VRAMç”¨äºopt-350m

2. **æ—¶é—´æˆæœ¬**:
   - è¾¾åˆ°90%ç¨€ç–åº¦å¯èƒ½éœ€è¦20+æ¬¡è¿­ä»£
   - æ¯æ¬¡è¿­ä»£3ä¸ªepoch,æ€»å…±60+ä¸ªepoch
   - é¢„è®¡éœ€è¦6-10å°æ—¶(å–å†³äºGPUå’Œbatch size)

3. **æ£€æŸ¥ç‚¹**:
   - æ¯æ¬¡è¿­ä»£åéƒ½ä¼šè¯„ä¼°perplexity
   - å»ºè®®ç›‘æ§è®­ç»ƒæ›²çº¿,åŠæ—¶å‘ç°é—®é¢˜
   - å¯ä»¥åœ¨ä¸­é€”åœæ­¢å¹¶ä½¿ç”¨å·²è¾¾åˆ°çš„ç¨€ç–åº¦

## ä»£ç é›†æˆ

å¦‚æœè¦åœ¨è‡ªå·±çš„ä»£ç ä¸­ä½¿ç”¨:

```python
from iterative_train import iterative_training_wrapper

config = {
    "model_name": "opt-125m",
    "target_sparsity": 0.9,
    "prune_percentage": 0.1,
    "epochs_per_iteration": 3,
    "max_iterations": 25,
    "batch_size": 8,
    "lr": 1e-5,
    "train_steps": 2500,
    "seed": 42,
    "save_model": True,
}

iterative_training_wrapper(config)
```

## ç›¸å…³è®ºæ–‡

1. **Iterative Magnitude Pruning** (Han et al., 2015)
   - "Learning both Weights and Connections for Efficient Neural Networks"

2. **Gradual Pruning** (Zhu & Gupta, 2017)
   - "To prune, or not to prune: exploring the efficacy of pruning for model compression"

3. **SparseGPTå¯¹æ¯”** (Frantar & Alistarh, 2023)
   - "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"
